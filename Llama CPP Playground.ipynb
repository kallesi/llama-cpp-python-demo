{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring possibilities of simplest ways to run local LLMs \n",
    "\n",
    "Tool in testing - LlamaCPP in python\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "- Llama CPP Python package - install by `pip install llama-cpp-python`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then, load the model (download from HF or other sources) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'falcon.embedding_length': '4544', 'general.name': 'Falcon', 'falcon.attention.head_count_kv': '1', 'general.architecture': 'falcon', 'falcon.context_length': '2048', 'falcon.tensor_data_layout': 'jploski', 'falcon.feed_forward_length': '18176', 'falcon.block_count': '32', 'falcon.attention.head_count': '71', 'falcon.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '11', 'general.file_type': '2', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "llm = Llama(model_path=r\"C:\\Users\\kalle\\Desktop\\Models\\GPT4All\\GPT4All Falcon\\gpt4all-falcon-newbpe-q4_0.gguf\")\n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Prompt (Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write me python code to read a csv file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of how you can read a CSV file using the pandas library in Python:\n",
      "```import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# create a sample CSV file\n",
      "data = {'Name': ['John', 'Jane', 'Bob', 'Alice'],\n",
      "        'Age': [27, 35, 42, 30],\n",
      "        'Gender': ['Male', 'Female', 'Male', 'Female']}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# read the CSV file\n",
      "df.head()```This code imports the pandas library and creates a sample CSV file with some data. The data is then read into a pandas DataFrame object. The `head()` method is used to display the first few rows of the DataFrame to verify that it has been successfully read.\n",
      "\n",
      "Here is the output of running this code:\n",
      "```\n",
      "Name    Age Gender\n",
      "0   John   27 Male\n",
      "1   Jane   35 Female\n",
      "2   Bob   42 Male\n",
      "3   Alice   30 Female\n",
      "```"
     ]
    }
   ],
   "source": [
    "stream = llm(\n",
    "    prompt=\"Question: \" + prompt + \" Answer:\" + \"\\n\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.8,\n",
    "    stop=[\"Question:\", \"Q:\", \"Answer\"],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for output in stream:\n",
    "    completion_fragment = copy.deepcopy(output)\n",
    "    print(completion_fragment[\"choices\"][0][\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
